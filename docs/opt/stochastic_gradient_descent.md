# 随机梯度下降 (Stochastic Gradient Descent, SGD)

## 1. 基本概念

随机梯度下降 (SGD) 是机器学习中最基础也是最重要的优化算法之一。它是梯度下降法的一个变种，通过使用随机采样的数据子集来计算梯度，从而在大型数据集上实现高效的参数更新。

## 2. 核心思想

SGD 的核心思想是：
- **随机采样**：每次迭代只使用一个或少量样本来计算梯度
- **在线学习**：可以处理流式数据，不需要存储整个数据集
- **计算效率**：相比批量梯度下降，计算复杂度大大降低
- **噪声引入**：随机性有助于跳出局部最优解

## 3. 算法原理

### 3.1 批量梯度下降 (BGD) 回顾

对于损失函数 $J(\theta) = \frac{1}{m}\sum_{i=1}^{m} L(f(x_i, \theta), y_i)$，批量梯度下降的更新规则为：

$$\theta_{t+1} = \theta_t - \alpha \nabla J(\theta_t) = \theta_t - \alpha \frac{1}{m}\sum_{i=1}^{m} \nabla L(f(x_i, \theta_t), y_i)$$

### 3.2 随机梯度下降 (SGD)

SGD 每次只使用一个样本计算梯度：

$$\theta_{t+1} = \theta_t - \alpha \nabla L(f(x_i, \theta_t), y_i)$$

其中 $(x_i, y_i)$ 是随机选择的样本。

### 3.3 小批量梯度下降 (Mini-batch SGD)

实际应用中通常使用小批量：

$$\theta_{t+1} = \theta_t - \alpha \frac{1}{b}\sum_{i \in B_t} \nabla L(f(x_i, \theta_t), y_i)$$

其中 $B_t$ 是第 $t$ 次迭代的小批量，$b$ 是批量大小。

## 4. 算法流程

### 4.1 伪代码

```
输入：学习率 α，批量大小 b，最大迭代次数 T
输入：初始参数 θ₀
初始化：t = 0

while t < T:
    # 随机采样小批量
    B_t = 随机采样 b 个样本
    
    # 计算小批量梯度
    g_t = (1/b) * Σ_{i∈B_t} ∇L(f(x_i, θ_t), y_i)
    
    # 更新参数
    θ_{t+1} = θ_t - α * g_t
    
    t = t + 1

输出：最终参数 θ
```

### 4.2 关键步骤说明

1. **数据采样**：从训练集中随机选择小批量数据
2. **梯度计算**：计算小批量上的平均梯度
3. **参数更新**：使用梯度更新模型参数
4. **重复迭代**：重复上述过程直到收敛

## 5. 学习率调度

### 5.1 固定学习率

最简单的策略是使用固定学习率，但可能导致：
- 学习率过大：震荡或发散
- 学习率过小：收敛过慢

### 5.2 学习率衰减

常用的学习率衰减策略：

#### 5.2.1 时间衰减
$$\alpha_t = \frac{\alpha_0}{1 + \gamma t}$$

#### 5.2.2 指数衰减
$$\alpha_t = \alpha_0 \cdot \gamma^t$$

#### 5.2.3 步长衰减
$$\alpha_t = \alpha_0 \cdot \gamma^{\lfloor t/s \rfloor}$$

其中 $s$ 是衰减步长。

#### 5.2.4 余弦退火
$$\alpha_t = \alpha_{min} + \frac{1}{2}(\alpha_{max} - \alpha_{min})(1 + \cos(\frac{t}{T}\pi))$$

## 6. 动量 (Momentum)

### 6.1 基本动量

为了加速收敛和减少震荡，引入动量项：

$$v_t = \beta v_{t-1} + \alpha \nabla L(\theta_t)$$
$$\theta_{t+1} = \theta_t - v_t$$

其中 $\beta$ 是动量系数（通常设为 0.9）。

### 6.2 Nesterov 动量

Nesterov 动量在计算梯度前先进行动量更新：

$$v_t = \beta v_{t-1} + \alpha \nabla L(\theta_t - \beta v_{t-1})$$
$$\theta_{t+1} = \theta_t - v_t$$

## 7. 优缺点分析

### 7.1 优点

1. **计算效率高**：每次迭代只需要计算小批量梯度
2. **内存需求低**：不需要存储整个数据集
3. **在线学习**：可以处理流式数据
4. **跳出局部最优**：随机性有助于探索解空间
5. **实现简单**：算法逻辑清晰，易于实现
6. **理论基础扎实**：收敛性理论完善

### 7.2 缺点

1. **收敛速度慢**：相比二阶方法收敛较慢
2. **超参数敏感**：对学习率设置敏感
3. **震荡问题**：可能在高维空间中震荡
4. **噪声影响**：随机性可能导致训练不稳定
5. **局部最优**：可能陷入局部最优解

## 8. 与其他优化器的比较

### 8.1 vs 批量梯度下降 (BGD)

| 特性 | BGD | SGD |
|------|-----|-----|
| 计算复杂度 | O(m) | O(b) |
| 内存需求 | 高 | 低 |
| 收敛速度 | 慢但稳定 | 快但可能震荡 |
| 并行化 | 困难 | 容易 |
| 在线学习 | 不支持 | 支持 |

### 8.2 vs Adam

| 特性 | SGD | Adam |
|------|-----|------|
| 学习率 | 需要调优 | 自适应 |
| 收敛速度 | 较慢 | 较快 |
| 超参数 | 学习率、动量 | 多个超参数 |
| 内存使用 | 低 | 中等 |
| 泛化能力 | 通常更好 | 可能过拟合 |

## 9. 实际应用

### 9.1 深度学习框架中的使用

```python
# PyTorch
import torch.optim as optim
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

# TensorFlow/Keras
from tensorflow.keras.optimizers import SGD
optimizer = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)
```

### 9.2 超参数调优建议

1. **学习率**：
   - 初始学习率：0.01 - 0.1
   - 使用学习率调度器
   - 根据验证集性能调整

2. **批量大小**：
   - 小数据集：32 - 128
   - 大数据集：128 - 512
   - 根据GPU内存调整

3. **动量系数**：
   - 通常设为 0.9
   - 可以尝试 0.95 或 0.99

## 10. 收敛性分析

### 10.1 理论保证

在凸优化设置下，SGD 具有以下性质：

1. **期望收敛**：$E[J(\theta_t)] \to J(\theta^*)$
2. **收敛速度**：$O(1/\sqrt{t})$ 的收敛率
3. **强凸情况**：$O(1/t)$ 的线性收敛率

### 10.2 非凸优化

在非凸优化中：
- SGD 收敛到稳定点
- 随机性有助于跳出鞍点
- 批量大小影响收敛性质

## 11. 改进方法

### 11.1 自适应学习率

- **AdaGrad**：累积梯度平方
- **RMSProp**：指数移动平均
- **Adam**：结合动量和自适应学习率

### 11.2 二阶方法

- **牛顿法**：使用海塞矩阵
- **拟牛顿法**：近似海塞矩阵
- **自然梯度**：使用费舍尔信息矩阵

### 11.3 正则化技术

- **权重衰减**：L2 正则化
- **Dropout**：随机失活
- **批量归一化**：标准化激活

## 12. 实现细节

### 12.1 数值稳定性

```python
def sgd_update(grad, param, lr, momentum=None, velocity=None):
    if momentum is not None and velocity is not None:
        # 动量更新
        velocity = momentum * velocity + lr * grad
        param = param - velocity
        return param, velocity
    else:
        # 标准 SGD
        param = param - lr * grad
        return param
```

### 12.2 并行化

- **数据并行**：将数据分布到多个设备
- **模型并行**：将模型分布到多个设备
- **梯度累积**：模拟大批量训练

## 13. 应用场景

### 13.1 适合使用 SGD 的情况

1. **大型数据集**：数据量很大，内存有限
2. **在线学习**：需要实时更新模型
3. **简单模型**：线性模型、浅层神经网络
4. **泛化要求高**：需要良好的泛化能力

### 13.2 不适合使用 SGD 的情况

1. **小数据集**：数据量小，可以使用批量方法
2. **复杂模型**：深层神经网络，需要自适应方法
3. **稀疏梯度**：梯度稀疏，需要特殊处理
4. **快速收敛**：需要快速收敛到最优解

## 14. 数学推导

### 14.1 收敛性证明（凸情况）

对于凸函数 $J(\theta)$，假设：
- 梯度有界：$||\nabla J(\theta)|| \leq G$
- 学习率满足：$\sum_{t=1}^{\infty} \alpha_t = \infty$ 且 $\sum_{t=1}^{\infty} \alpha_t^2 < \infty$

则 SGD 收敛到最优解：

$$E[J(\theta_t)] - J(\theta^*) \leq \frac{G^2 \sum_{i=1}^{t} \alpha_i^2}{2\sum_{i=1}^{t} \alpha_i}$$

### 14.2 非凸情况

在非凸优化中，SGD 收敛到稳定点的概率为 1，即：

$$P(\lim_{t \to \infty} ||\nabla J(\theta_t)|| = 0) = 1$$

## 15. 总结

随机梯度下降是机器学习中最基础也是最重要的优化算法：

### 15.1 核心特点
- **简单有效**：算法简单，理论基础扎实
- **计算高效**：适合大规模数据处理
- **通用性强**：适用于各种机器学习任务

### 15.2 发展趋势
- **自适应方法**：Adam、AdaGrad 等自适应优化器
- **二阶方法**：牛顿法、拟牛顿法的改进
- **分布式训练**：大规模并行化实现

### 15.3 应用建议
- 对于简单任务，SGD 仍然是最佳选择
- 对于复杂任务，可以结合动量和自适应学习率
- 根据具体问题选择合适的优化器和超参数

SGD 虽然简单，但在实际应用中仍然具有重要价值，特别是在需要良好泛化能力的场景中。
